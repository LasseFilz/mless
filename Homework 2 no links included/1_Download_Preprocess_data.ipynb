{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f13b4a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/maschu09/mless/blob/main/time_series_forecasting/1_Download_Preprocess_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb8933",
   "metadata": {
    "id": "93fb8933"
   },
   "source": [
    "# Data download and preparation\n",
    "\n",
    "In the following, we will set-up a typical data preprocessing workflow for (environmental) timeseries data. It is quite common that such data can be obtained via a web interface (REST API) and that the output from this API requires substantial reformatting and rearranging before the data can be imported into analysis or forecasting models. Typical issues encountered include:\n",
    "* time series at different stations (and for different parameters) vary in length\n",
    "* timeseries have some or many missing values, which are either absent from the data series or encoded as special values\n",
    "* data vary in quality (some datasets may have jumps or awkward \"features\" that cannot be explained by natural phenomena)\n",
    "* metadata to inform the data selection are missing ort difficult to access/process\n",
    "\n",
    "In this notebook, we try to keep things relatively simple and straightforward while providing a set of tools that can also be used in more meaningful applications.\n",
    "\n",
    "The default use case on which this notebook builds is to extract timeseries of the variable \"temperature\" from a handful of measurement locations, store intermediate results to avoid repeated downloads, and package the data into a single CSV file for input in the statistical analyses and ML models shown in the subsequent notebooks. The code will also run if you extract several variables at once or increase the number of stations. However, for massive data downloads, you will need to rewrite the code to be more efficient and add some monitoring of your downloads.\n",
    "\n",
    "> Please note: In case of runtime loss or a need to run any segmented sections of the code make sure to run all the housekeeping cells before it\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13766b33",
   "metadata": {
    "id": "13766b33"
   },
   "source": [
    "## Initial Setup and Data download\n",
    "> This section downloads example data from TOAR for 5 stations in Germany. Refer to [TOAR Quick UserGuide](https://toar-data.fz-juelich.de/sphinx/TOAR_UG_Vol02_Quick_Start/build/html/examples.html) examples to better understand data structuring in TOAR that is used in the below snippet to download examples.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/maschu09/mless/blob/main/time_series_forecasting/TOARFetchingBlockDiag.png?raw=1\" alt=\"TOAR Block Diag\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb9d93",
   "metadata": {
    "id": "49eb9d93"
   },
   "source": [
    "### Housekeeping: Initial setup, declarations and method definitions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "cb98c436",
   "metadata": {
    "id": "cb98c436"
   },
   "outputs": [],
   "source": [
    "# Most notebook servers like google collab should have these packages pre-installed\n",
    "# In such cases this is just a sanity check\n",
    "#!pip install pandas numpy requests tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4eba776",
   "metadata": {
    "id": "b4eba776"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de7d59",
   "metadata": {
    "id": "46de7d59"
   },
   "source": [
    "#### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc809476",
   "metadata": {
    "id": "fc809476"
   },
   "outputs": [],
   "source": [
    "# Mount google drive when working in colab\n",
    "hasCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "if hasCOLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  BASEPATH = '/content/drive/MyDrive'\n",
    "else:\n",
    "  BASEPATH = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100829ff",
   "metadata": {
    "id": "100829ff"
   },
   "outputs": [],
   "source": [
    "#!ls drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51ceddc",
   "metadata": {
    "id": "e51ceddc"
   },
   "outputs": [],
   "source": [
    "# Global constants\n",
    "TIMESERIES_DATA_DIR = BASEPATH + \"/timeseries_data/\"\n",
    "TIMESERIES_CSV_DIR = os.path.join(TIMESERIES_DATA_DIR, \"toar_csv_timeseries\")\n",
    "TIMESERIES_ID_FILE = os.path.join(TIMESERIES_DATA_DIR, \"timeseriesIDs.json\")\n",
    "MIN_FILE_SIZE_BYTES = 100\n",
    "group_columns = ['station_code', 'latitude', 'longitude']\n",
    "\n",
    "os.makedirs(TIMESERIES_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TIMESERIES_CSV_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af26a1",
   "metadata": {
    "id": "46af26a1"
   },
   "source": [
    "### Custom data selection for experiments\n",
    "\n",
    "The station codes in the example snippet below are checked for a common daterange for the chosen variables and offer reasonably complete timeseries. If you prefer to experiment with other datasets, consult the documentation on the [Search API](https://toar-data.fz-juelich.de/api/v2/#search-combined-endpoint-of-stations-and-timeseries).\n",
    "\n",
    "ðŸ˜ˆ **Question 1:** What are common reasons for gaps in observational data records, such as from TOAR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9663fef6",
   "metadata": {
    "id": "9663fef6"
   },
   "outputs": [],
   "source": [
    "# German stations with good data coverage\n",
    "station_codes = [\"DENW094\", \"DEBW073\",\"DEHE020\"]\n",
    "variable_columns = [\"temp\"]\n",
    "# station_codes = [\"DENW094\"]\n",
    "# variable_columns = [\"no2\", \"temp\", \"o3\", \"no\", \"press\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d99569",
   "metadata": {
    "id": "70d99569"
   },
   "source": [
    "ðŸ˜ˆ **Task 1:** Explore the `station_codes` variable and try changing the station(s) to a different region.\n",
    "\n",
    "Below methods each have appropriate documentation and comments to illustrate the logical flow *(they are placed with enough safeguards against both the API and optimized to avoid re-downloads when interupted during partial downloads to accomodate any loss of runtime on platforms like google colab)* and briefly described here for ease of use\n",
    "\n",
    "ðŸ˜ˆ **Task 2:** Inspect the function `pivot_handle()`. What does it return, and why is pivoting important for time series analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2eebb7",
   "metadata": {
    "id": "ef2eebb7"
   },
   "outputs": [],
   "source": [
    "def load_existing_timeseries_ids():\n",
    "    \"\"\"\n",
    "    Load existing timeseries IDs from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing stored timeseries metadata.\n",
    "    \"\"\"\n",
    "    return json.load(open(TIMESERIES_ID_FILE, 'r')) if os.path.exists(TIMESERIES_ID_FILE) else {}\n",
    "\n",
    "def save_timeseries_ids(timeseries_data):\n",
    "    \"\"\"\n",
    "    Save timeseries metadata to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        timeseries_data (dict): A dictionary containing timeseries metadata.\n",
    "    \"\"\"\n",
    "    json.dump(timeseries_data, open(TIMESERIES_ID_FILE, 'w'), indent=4)\n",
    "\n",
    "def fetch_timeseries_data(station_codes, existing_timeseries, variable_columns):\n",
    "    \"\"\"\n",
    "    Fetch timeseries metadata for given station codes, filtering by specified variables.\n",
    "\n",
    "    Args:\n",
    "        station_codes (list): List of station codes to fetch data for.\n",
    "        existing_timeseries (dict): Dictionary of previously fetched timeseries metadata.\n",
    "        variable_columns (list): List of variable names to retain.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary containing filtered timeseries metadata.\n",
    "    \"\"\"\n",
    "    base_url = \"http://toar-data.fz-juelich.de/api/v2/search/?codes=\"\n",
    "    unique_entries = existing_timeseries.copy()\n",
    "    processed_station_codes = {details['station_code'] for details in existing_timeseries.values()}\n",
    "\n",
    "    for code in station_codes:\n",
    "        if code in processed_station_codes:\n",
    "            print(f\"\\t\\tStation {code} is already processed, skipping.\")\n",
    "            continue\n",
    "\n",
    "        response = requests.get(base_url + code, timeout=1000)\n",
    "        if response.status_code == 200:\n",
    "            for entry in response.json():\n",
    "                if (variable_name := entry.get('variable', {}).get('name')) in variable_columns:\n",
    "                    timeseries_id = entry.get('id')\n",
    "                    if timeseries_id not in unique_entries:\n",
    "                        unique_entries[timeseries_id] = {\n",
    "                            'data_start_date': entry.get('data_start_date'),\n",
    "                            'data_end_date': entry.get('data_end_date'),\n",
    "                            'variable_name': variable_name,\n",
    "                            'station_code': code,\n",
    "                            'latitude': entry.get('station', {}).get('coordinates', {}).get('lat'),\n",
    "                            'longitude': entry.get('station', {}).get('coordinates', {}).get('lng'),\n",
    "                        }\n",
    "        else:\n",
    "            print(f\"\\t\\tFailed to fetch data for station {code}. Status code: {response.status_code}\")\n",
    "    return unique_entries\n",
    "\n",
    "def pivot_handle(dfs, metadata_columns, variable_columns):\n",
    "    \"\"\"\n",
    "    Pivot and structure the timeseries dataframe for sequential data analysis.\n",
    "\n",
    "    Args:\n",
    "        dfs (pd.DataFrame): Dataframe containing timeseries data.\n",
    "        metadata_columns (list): List of metadata column names.\n",
    "        variable_columns (list): List of variable names to include.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with pivoted structure.\n",
    "    \"\"\"\n",
    "    pivot_df = dfs.pivot_table(index='datetime', columns='variable_name', values='value', aggfunc='mean')\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    print(f\"Station {dfs['station_code'].unique()} min time: {dfs['datetime'].min()}, max time: {dfs['datetime'].max()}, hours between: {(dfs['datetime'].max() - dfs['datetime'].min()) / pd.Timedelta(hours=1):.2f}\")\n",
    "    reference_index = pd.date_range(start=dfs['datetime'].min(), end=dfs['datetime'].max(), freq=\"h\", tz=\"UTC\")\n",
    "    reference_df = pd.DataFrame({'datetime': reference_index})\n",
    "\n",
    "    pivot_df_ = reference_df.merge(pivot_df, on='datetime', how='left')\n",
    "\n",
    "    for col in metadata_columns:\n",
    "        if dfs[col].notna().any():\n",
    "            value = dfs[col].dropna().iloc[0]\n",
    "            pivot_df_.insert(0, col, value)\n",
    "        else:\n",
    "            print(f\"Station {dfs['station_code'].unique()}: metadata {col} has no value\")\n",
    "\n",
    "    return pivot_df_\n",
    "\n",
    "def download_csv_data(timeseries_data, variable_columns):\n",
    "    \"\"\"\n",
    "    Download and process CSV data for each timeseries ID.\n",
    "\n",
    "    Args:\n",
    "        timeseries_data (dict): Dictionary containing timeseries metadata.\n",
    "        variable_columns (list): List of variable names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataframe of all timeseries data.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    metadata_columns = ['station_code', 'latitude', 'longitude']\n",
    "\n",
    "    for ts_id, details in timeseries_data.items():\n",
    "        csv_path = os.path.join(TIMESERIES_CSV_DIR, f\"{ts_id}.csv\")\n",
    "\n",
    "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > MIN_FILE_SIZE_BYTES:\n",
    "            print(f\"\\tCSV already exists for timeseries ID {ts_id}, skipping download.\")\n",
    "        else:\n",
    "            print(f\"\\tDownloading data for timeseries ID {ts_id}\")\n",
    "            url = f\"http://toar-data.fz-juelich.de/api/v2/data/timeseries/{ts_id}?format=csv\"\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=1000)\n",
    "                response.raise_for_status()\n",
    "                with open(csv_path, 'wb') as file:\n",
    "                    file.writelines(response.iter_content(chunk_size=8192))\n",
    "                print(f\"\\t\\tRaw data CSV of {ts_id} saved: {csv_path}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\t\\tFailed to download data for timeseries ID {ts_id}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, skiprows=lambda i: i < next(i for i, line in enumerate(open(csv_path)) if line.startswith('datetime')), low_memory=False)\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')\n",
    "            df[['variable_name', 'station_code', 'latitude', 'longitude']] = details['variable_name'], details['station_code'], details['latitude'], details['longitude']\n",
    "            print(f\"Dataframe for timeseries ID {ts_id} loaded successfully with shape {df.shape}\")\n",
    "            dataframes.append(pivot_handle(df, metadata_columns, variable_columns))\n",
    "        except (pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
    "            print(f\"\\tError processing CSV for timeseries ID {ts_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True).sort_values(by=['station_code', 'datetime']) if dataframes else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab0925",
   "metadata": {
    "id": "6dab0925"
   },
   "source": [
    "### Download via REST API\n",
    "\n",
    "ðŸ˜ˆ **Task 3:** Try downloading a different variable or add another pollutant (e.g., `so2`). What changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "5c3464c4",
   "metadata": {
    "id": "5c3464c4",
    "outputId": "32aa6df6-b53a-47c9-f150-dca20e15ce0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStation DENW094 is already processed, skipping.\n",
      "\t\tStation DEBW073 is already processed, skipping.\n",
      "\t\tStation DEHE020 is already processed, skipping.\n",
      "\t Number of time series meta data fetched : 3\n",
      "\tCSV already exists for timeseries ID 76, skipping download.\n",
      "Dataframe for timeseries ID 76 loaded successfully with shape (246552, 9)\n",
      "Station ['DENW094'] min time: 1997-01-01 00:00:00+00:00, max time: 2025-06-27 07:00:00+00:00, hours between: 249703.00\n",
      "\tCSV already exists for timeseries ID 22639, skipping download.\n",
      "Dataframe for timeseries ID 22639 loaded successfully with shape (110890, 9)\n",
      "Station ['DEBW073'] min time: 1997-01-01 00:00:00+00:00, max time: 2011-12-31 23:00:00+00:00, hours between: 131471.00\n",
      "\tCSV already exists for timeseries ID 18022, skipping download.\n",
      "Dataframe for timeseries ID 18022 loaded successfully with shape (220677, 9)\n",
      "Station ['DEHE020'] min time: 1997-01-01 00:00:00+00:00, max time: 2025-06-18 07:00:00+00:00, hours between: 249487.00\n",
      "\t Total dataFrames processed : 630664 and shape of first dataframe (630664, 5).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249704</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249705</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249706</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249707</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249708</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        longitude   latitude station_code                  datetime  temp\n",
       "249704   7.567796  47.819182      DEBW073 1997-01-01 00:00:00+00:00 -10.0\n",
       "249705   7.567796  47.819182      DEBW073 1997-01-01 01:00:00+00:00 -11.0\n",
       "249706   7.567796  47.819182      DEBW073 1997-01-01 02:00:00+00:00 -11.0\n",
       "249707   7.567796  47.819182      DEBW073 1997-01-01 03:00:00+00:00 -12.0\n",
       "249708   7.567796  47.819182      DEBW073 1997-01-01 04:00:00+00:00 -12.0"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load existing timeseries IDs from json to skip calls to TOAR\n",
    "existing_timeseries = load_existing_timeseries_ids()\n",
    "\n",
    "timeseries_data = fetch_timeseries_data(station_codes, existing_timeseries,variable_columns)\n",
    "print(f\"\\t Number of time series meta data fetched : {len(timeseries_data)}\")\n",
    "\n",
    "# save existing timeseries IDs as json to reduce calls to TOAR in future\n",
    "save_timeseries_ids(timeseries_data)\n",
    "\n",
    "dataframes = download_csv_data(timeseries_data,variable_columns)\n",
    "print(f\"\\t Total dataFrames processed : {len(dataframes)} and shape of first dataframe {dataframes.shape}.\")\n",
    "\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8d25f",
   "metadata": {
    "id": "86c8d25f"
   },
   "source": [
    "Most of the data in TOAR is observational data which has few or many data gaps. The \"temp\"(erature) data that you downloaded is actually from a model (ERA5 reanalysis) and should therefore not have any missing values (except if your requested daterange extends beyond the period for which we extracted ERA5 data).\n",
    "\n",
    "To demonstrate a more typical workflow of handling observational data, we will now inspect, interpolate, and remove missing data values. Note that the pivot routine above merged data from different variables together and introduced NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588a2f2",
   "metadata": {
    "id": "2588a2f2"
   },
   "source": [
    "## Data handling (observational gaps)\n",
    "\n",
    "ðŸ˜ˆ **Task 4:** Insert some code to find out how many NaN values each variable in the dataset contains. Even better: try to generate a bargraph showing the number of instances with consecutive NaN values (1, 2, 3, ..., more than 10).\n",
    "\n",
    "To provide as much training data to ML models as possible, you typically want to fill gaps through interpolation. For environmental data, it is common practice to interpolate over up to 6 hours but not longer. In many cases, linear interpolation will work just fine.\n",
    "\n",
    "\n",
    "ðŸ˜ˆ **Question 3:** Why is it acceptable to fill up to 6 missing hourly values in this dataset?\n",
    "\n",
    "ðŸ˜ˆ **Question 4:** Why do you need to interpolate at all? Wouldn't it be sufficient to randomly sample from the existing data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0237f955",
   "metadata": {
    "id": "0237f955"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fill_six_nans(group):\n",
    "    \"\"\"\n",
    "    Fills up to six consecutive NaN values in a given pandas Series using linear interpolation\n",
    "    if the NaNs are surrounded by valid values. If the NaNs are at the start, they are replaced\n",
    "    with zeros, and if they are at the end, they are filled with the last known value.\n",
    "\n",
    "    Args:\n",
    "        group (pd.Series): The input Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series where up to six consecutive NaNs are interpolated, and longer NaN\n",
    "        sequences are partially filled while preserving the original index.\n",
    "    \"\"\"\n",
    "    values = group.to_numpy()\n",
    "    i = 0\n",
    "    while i < len(values):\n",
    "        if np.isnan(values[i]):\n",
    "            start = i\n",
    "            while i < len(values) and np.isnan(values[i]):\n",
    "                i += 1\n",
    "            end = min(i, start + 6)  # Limit to filling only 6 NaNs\n",
    "\n",
    "            if start > 0 and i < len(values):  # NaNs in the middle\n",
    "                fill_values = np.linspace(values[start - 1], values[i], end - start + 2)[1:-1]\n",
    "            elif start == 0:  # NaNs at the start\n",
    "                fill_values = [0] * (end - start)\n",
    "            elif i >= len(values):  # NaNs at the end\n",
    "                fill_values = [values[start - 1]] * (end - start)\n",
    "            values[start:end] = fill_values\n",
    "        else:\n",
    "            i += 1\n",
    "    return pd.Series(values, index=group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d861e5dd",
   "metadata": {
    "id": "d861e5dd",
    "outputId": "72ec71f5-cdce-4dc1-d694-f446894d9f34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "station_code          0\n",
       "datetime              0\n",
       "o3              1316760\n",
       "no2             1317706\n",
       "no              1319226\n",
       "temp              62161\n",
       "press           1367210\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[variable_columns] = dataframes.groupby(group_columns)[variable_columns].transform(fill_six_nans)\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c220f",
   "metadata": {
    "id": "c67c220f"
   },
   "source": [
    "Now rest of the Nas can be dropped as that station might not have data collected in the time period and the data needs to be normalized.\n",
    "\n",
    "ðŸ˜ˆ **Question 5:** What risks might arise if normalization is applied *before* handling missing values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "079160de",
   "metadata": {
    "id": "079160de",
    "outputId": "df195013-3e82-4da9-ee31-810829f430f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude       0\n",
       "latitude        0\n",
       "station_code    0\n",
       "datetime        0\n",
       "o3              0\n",
       "no2             0\n",
       "no              0\n",
       "temp            0\n",
       "press           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = dataframes.dropna()\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2b6ef96",
   "metadata": {
    "id": "e2b6ef96",
    "outputId": "14550138-169f-4ea7-9962-f4739424e8ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618005c",
   "metadata": {
    "id": "3618005c"
   },
   "source": [
    "## Staged data loading (Housekeeping)\n",
    "\n",
    "Finally, and before we want to perform any analysis of the data, we will store the \"cleaned\" dataset for later re-use. Subsequent notebooks can then easily reload the \"raw_data\" from the local storage or your mounted google drive folder when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6e51c3f",
   "metadata": {
    "id": "e6e51c3f"
   },
   "outputs": [],
   "source": [
    "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838444Ie5Lsr",
   "metadata": {
    "id": "838444Ie5Lsr"
   },
   "source": [
    "ðŸ˜ˆ **Task 6:** Re-inspect the code above and reflect on the data management strategy employed here. What happens if you re-run the script after changing station codes or variables? Would the notebook repeat all download operations if you add a single station or variable? What could you do to improve this workflow?\n",
    "\n",
    "Don't waste time by trying to build the ultimate do-it-all dataloader - you can attend my lecture on Earth System Data Processing if you are keen to go deeper down this route."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c18223",
   "metadata": {
    "id": "45c18223"
   },
   "source": [
    "### Leftovers from first cleanup\n",
    "\n",
    "\n",
    "Below cell can be used to reload data if using an open source notebook servers like google colab and the if the usage limit is reached or for other issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "626bfaba",
   "metadata": {
    "id": "626bfaba",
    "outputId": "5f845c54-d6eb-40f2-8bbf-dcc5a1138ade"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.567796</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude   latitude station_code                  datetime  temp\n",
       "0   7.567796  47.819182      DEBW073 1997-01-01 00:00:00+00:00 -10.0\n",
       "1   7.567796  47.819182      DEBW073 1997-01-01 01:00:00+00:00 -11.0\n",
       "2   7.567796  47.819182      DEBW073 1997-01-01 02:00:00+00:00 -11.0\n",
       "3   7.567796  47.819182      DEBW073 1997-01-01 03:00:00+00:00 -12.0\n",
       "4   7.567796  47.819182      DEBW073 1997-01-01 04:00:00+00:00 -12.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## Raw data csv is also made available for the select stations in URL:\n",
    "url = \"https://drive.google.com/uc?export=download&id=1cmTTWY3f18SikgRBcZzhtFswIf7XwPJq\"\n",
    "dataframes = pd.read_csv(url,parse_dates=[\"datetime\"])\n",
    "## Else if using local files:\n",
    "# dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"))\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9d4b72a",
   "metadata": {
    "id": "b9d4b72a",
    "outputId": "558d69c8-8e4e-4df4-dd9b-9f4df088a0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579480, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44b4e968",
   "metadata": {
    "id": "44b4e968",
    "outputId": "78e2a0da-416a-4421-dbea-8b82f6116367"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude       0\n",
       "latitude        0\n",
       "station_code    0\n",
       "datetime        0\n",
       "temp            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ZLNbEWgo7c0",
   "metadata": {
    "id": "5ZLNbEWgo7c0"
   },
   "source": [
    "### (Optional/Advanced) Multi-Variable Case:\n",
    "\n",
    "We've seen how to download one variable for multiple stations. Now let's try and download multi-variables for one station.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8CVdDesp9bq",
   "metadata": {
    "id": "r8CVdDesp9bq"
   },
   "source": [
    "- You know the drill at this point:\n",
    "  - Set the paths for loading the timeseries_ids(unique to each variable) corresponding to the station code and downloading timeseries data for all the variables.\n",
    "  - Fetch the variables for the station of interest\n",
    "  - Fill the gaps upto 6hrs\n",
    "  - Drop NAs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tVyo57nMqOd4",
   "metadata": {
    "id": "tVyo57nMqOd4"
   },
   "source": [
    "#### Fetch the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "Mk8UUqZOqN05",
   "metadata": {
    "id": "Mk8UUqZOqN05"
   },
   "outputs": [],
   "source": [
    "# Let us focus on one single station\n",
    "station_codes = [\"DENW094\"]\n",
    "#variable_columns = [\"no2\", \"temp\", \"o3\", \"no\", \"press\"]\n",
    "variable_columns = [\"temp\", \"o3\"]\n",
    "# Note: the TOAR API sets limits to anonymous users; you can download max 3 timeseries with one request.\n",
    "# Hence, you will need to adapt this code to load data of all 5 variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FvPc3lp8aoN9",
   "metadata": {
    "id": "FvPc3lp8aoN9"
   },
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BBpWNtA2pX5K",
   "metadata": {
    "id": "BBpWNtA2pX5K"
   },
   "source": [
    "#### Download the variables data for that 1 station via REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "yIGfmD85pMF7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "yIGfmD85pMF7",
    "outputId": "f69a57a6-3ef4-4cc6-b110-d7038f759867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStation DENW094 is already processed, skipping.\n",
      "\t Number of time series meta data fetched : 2\n",
      "\tCSV already exists for timeseries ID 73, skipping download.\n",
      "Dataframe for timeseries ID 73 loaded successfully with shape (211247, 9)\n",
      "Station ['DENW094'] min time: 1999-07-02 18:00:00+00:00, max time: 2025-06-27 07:00:00+00:00, hours between: 227797.00\n",
      "\tCSV already exists for timeseries ID 76, skipping download.\n",
      "Dataframe for timeseries ID 76 loaded successfully with shape (246552, 9)\n",
      "Station ['DENW094'] min time: 1997-01-01 00:00:00+00:00, max time: 2025-06-27 07:00:00+00:00, hours between: 249703.00\n",
      "\t Total dataFrames processed : 477502 and shape of first dataframe (477502, 6).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>o3</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227798</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227799</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227800</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227801</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227802</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        longitude   latitude station_code                  datetime  o3   temp\n",
       "227798   6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 NaN -15.85\n",
       "227799   6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 NaN -16.35\n",
       "227800   6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 NaN -16.45\n",
       "227801   6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 NaN -17.15\n",
       "227802   6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 NaN -17.35"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load existing timeseries IDs from json to skip calls to TOAR\n",
    "existing_timeseries = load_existing_timeseries_ids()\n",
    "\n",
    "timeseries_data = fetch_timeseries_data(station_codes, existing_timeseries,variable_columns)\n",
    "print(f\"\\t Number of time series meta data fetched : {len(timeseries_data)}\")\n",
    "\n",
    "# save existing timeseries IDs as json to reduce calls to TOAR in future\n",
    "save_timeseries_ids(timeseries_data)\n",
    "\n",
    "dataframes_original = download_csv_data(timeseries_data,variable_columns)\n",
    "print(f\"\\t Total dataFrames processed : {len(dataframes_original)} and shape of first dataframe {dataframes_original.shape}.\")\n",
    "\n",
    "dataframes_original.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqmumS-_pope",
   "metadata": {
    "id": "qqmumS-_pope"
   },
   "source": [
    "#### Data handling (observational gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7xwW3dEKftO",
   "metadata": {
    "id": "s7xwW3dEKftO"
   },
   "source": [
    "##### Filling up upto 6 NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7c271e10-248f-4f33-bb0a-ab69b8ef8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>o3</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227798</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227799</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227800</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227801</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227802</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        longitude   latitude station_code                  datetime  o3   temp\n",
       "227798   6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 NaN -15.85\n",
       "227799   6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 NaN -16.35\n",
       "227800   6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 NaN -16.45\n",
       "227801   6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 NaN -17.15\n",
       "227802   6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 NaN -17.35"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframes = dataframes_original.drop(columns=[\"no2\", \"press\", \"no\"])\n",
    "dataframes = dataframes_original\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c213b49d-b708-4c7c-85a1-8a478bfc19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fill_six_nans2(group):\n",
    "    \"\"\"\n",
    "    Fills up to six consecutive NaN values in a given pandas Series using linear interpolation\n",
    "    if the NaNs are surrounded by valid values. If the NaNs are at the start, they are replaced\n",
    "    with zeros, and if they are at the end, they are filled with the last known value.\n",
    "\n",
    "    Args:\n",
    "        group (pd.Series): The input Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series where up to six consecutive NaNs are interpolated, and longer NaN\n",
    "        sequences are partially filled while preserving the original index.\n",
    "    \"\"\"\n",
    "    values = group.to_numpy()\n",
    "    i = 0\n",
    "    while i < len(values):\n",
    "        if np.isnan(values[i]):\n",
    "            start = i\n",
    "            while i < len(values) and np.isnan(values[i]):\n",
    "                i += 1\n",
    "            end = min(i, start + 6)  # Limit to filling only 6 NaNs\n",
    "\n",
    "            if start > 0 and i < len(values):  # NaNs in the middle\n",
    "                fill_values = np.linspace(values[start - 1], values[i], end - start + 2)[1:-1]\n",
    "            elif start == 0:  # NaNs at the start\n",
    "                fill_values = [0] * (end - start)\n",
    "            elif i >= len(values):  # NaNs at the end\n",
    "                fill_values = [values[start - 1]] * (end - start)\n",
    "            values[start:end] = fill_values\n",
    "        else:\n",
    "            i += 1\n",
    "    return pd.Series(values, index=group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "91b3dfa6-eaaa-4d6d-8e16-5e0cd2ba6999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude            0\n",
       "latitude             0\n",
       "station_code         0\n",
       "datetime             0\n",
       "o3              266255\n",
       "temp            230950\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d2c5beb-0fae-47cd-933f-ed52a6cf8fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477502, 6)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17654985-034c-4214-8e26-25e4462c50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        longitude   latitude station_code                  datetime    temp  \\\n",
      "0        6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 -15.850   \n",
      "1        6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 -16.350   \n",
      "2        6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 -16.450   \n",
      "3        6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 -17.150   \n",
      "4        6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 -17.350   \n",
      "...           ...        ...          ...                       ...     ...   \n",
      "249699   6.093923  50.754704      DENW094 2025-06-27 03:00:00+00:00  18.200   \n",
      "249700   6.093923  50.754704      DENW094 2025-06-27 04:00:00+00:00  17.987   \n",
      "249701   6.093923  50.754704      DENW094 2025-06-27 05:00:00+00:00  17.798   \n",
      "249702   6.093923  50.754704      DENW094 2025-06-27 06:00:00+00:00  16.891   \n",
      "249703   6.093923  50.754704      DENW094 2025-06-27 07:00:00+00:00  15.351   \n",
      "\n",
      "               o3  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "...           ...  \n",
      "249699  33.504385  \n",
      "249700  35.478770  \n",
      "249701  36.230630  \n",
      "249702  36.546411  \n",
      "249703  32.048784  \n",
      "\n",
      "[249704 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grouping columns\n",
    "group_cols = ['longitude', 'latitude', 'station_code', 'datetime']\n",
    "\n",
    "# Aggregate with mean (NaNs ignored automatically)\n",
    "agg_dict = {\n",
    "    'temp': 'mean',\n",
    "    'o3': 'mean'\n",
    "}\n",
    "# we have lots of duplicate data regarding (datetime ^ station ^ latitude ^ longitude), here we aggregate them and remove the duplicates, \n",
    "# by taking the mean value of duplicates moving forward\n",
    "dataframes = dataframes.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "73e92deb-403f-4388-9d7d-f770643de054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates just in case, we do not have duplicates at this point though\n",
    "dataframes = dataframes.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "513a2394-060d-4845-a6cc-11b5cb82f784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude           0\n",
       "latitude            0\n",
       "station_code        0\n",
       "datetime            0\n",
       "temp             3152\n",
       "o3              38457\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bf1fb2a7-380e-4eae-89cb-3143bbfac3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249704, 6)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aacb0e10-c9f9-4efe-9c9b-2c7be9ead303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of time skips (hours):\n",
      "datetime\n",
      "1.0    249703\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#simple check how often 1, 2, 3, .. hours time difference appears in the data\n",
    "df = dataframes\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(['station_code', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "# Compute time differences in hours\n",
    "time_diffs = df.groupby('station_code')['datetime'].diff()\n",
    "time_diffs_hours = time_diffs.dt.total_seconds() / 3600\n",
    "\n",
    "# Count how often each skip length occurs\n",
    "histogram = time_diffs_hours.value_counts().sort_index()\n",
    "print(\"Histogram of time skips (hours):\")\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "sdTokb2GKeDH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "sdTokb2GKeDH",
    "outputId": "6b1391b2-ff69-4611-c7dd-5818c974e9d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude           0\n",
       "latitude            0\n",
       "station_code        0\n",
       "datetime            0\n",
       "temp             2090\n",
       "o3              27565\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing values by interpolation\n",
    "dataframes[variable_columns] = dataframes.groupby(group_columns)[variable_columns].transform(fill_six_nans2)\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Y9nlWqmJQe2",
   "metadata": {
    "id": "3Y9nlWqmJQe2"
   },
   "source": [
    "##### After dropping NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a1b82ec3-8a85-4672-ba78-69593d54a9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249704, 6)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "U5S7KBXSJOb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "U5S7KBXSJOb4",
    "outputId": "8ce3d5f1-9d18-4ab3-f49f-a15841f246fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude       0\n",
       "latitude        0\n",
       "station_code    0\n",
       "datetime        0\n",
       "temp            0\n",
       "o3              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing values\n",
    "dataframes = dataframes.dropna()\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qd4YM9V0Smu8",
   "metadata": {
    "id": "qd4YM9V0Smu8"
   },
   "source": [
    "#### Statistical Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7afc109c-2c70-4b39-a581-28c9a75d9562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of time skips (hours):\n",
      "datetime\n",
      "1.0        221354\n",
      "2.0            20\n",
      "3.0            20\n",
      "4.0            15\n",
      "5.0             6\n",
      "6.0             8\n",
      "7.0             7\n",
      "8.0             5\n",
      "9.0             4\n",
      "10.0            4\n",
      "11.0            7\n",
      "12.0            1\n",
      "13.0           10\n",
      "14.0            2\n",
      "15.0            3\n",
      "16.0            2\n",
      "17.0            3\n",
      "18.0            1\n",
      "19.0            4\n",
      "20.0            2\n",
      "21.0           30\n",
      "22.0            4\n",
      "23.0            2\n",
      "24.0            4\n",
      "25.0            1\n",
      "26.0            1\n",
      "27.0            4\n",
      "29.0            1\n",
      "30.0            1\n",
      "32.0            1\n",
      "33.0            1\n",
      "39.0            1\n",
      "41.0            2\n",
      "42.0            1\n",
      "43.0            1\n",
      "46.0            2\n",
      "49.0            1\n",
      "51.0            1\n",
      "52.0            1\n",
      "53.0            2\n",
      "55.0            1\n",
      "56.0            2\n",
      "59.0            2\n",
      "61.0            1\n",
      "63.0            1\n",
      "64.0            1\n",
      "70.0            1\n",
      "74.0            1\n",
      "78.0            1\n",
      "83.0            2\n",
      "84.0            1\n",
      "85.0            1\n",
      "121.0           1\n",
      "153.0           1\n",
      "155.0           1\n",
      "192.0           1\n",
      "250.0           1\n",
      "285.0           1\n",
      "1651.0          1\n",
      "21901.0         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#simple check how often 1, 2, 3, .. hours time difference appears in the data\n",
    "df = dataframes\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(['station_code', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "# Compute time differences in hours\n",
    "time_diffs = df.groupby('station_code')['datetime'].diff()\n",
    "time_diffs_hours = time_diffs.dt.total_seconds() / 3600\n",
    "\n",
    "# Count how often each skip length occurs\n",
    "histogram = time_diffs_hours.value_counts().sort_index()\n",
    "print(\"Histogram of time skips (hours):\")\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db7af959-d74b-4812-9cb8-62cd86d8cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>-15.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>-16.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>-16.45</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>-17.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude   latitude station_code                  datetime   temp   o3\n",
       "0   6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 -15.85  0.0\n",
       "1   6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 -16.35  0.0\n",
       "2   6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 -16.45  0.0\n",
       "3   6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 -17.15  0.0\n",
       "4   6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 -17.35  0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "61S7XC7zNU1y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "61S7XC7zNU1y",
    "outputId": "f5e2396b-1dba-4be0-9bef-02bbff2c6ba8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">temp</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "      <th>median</th>\n",
       "      <th>prod</th>\n",
       "      <th>nunique</th>\n",
       "      <th>5th_percentile</th>\n",
       "      <th>...</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "      <th>median</th>\n",
       "      <th>prod</th>\n",
       "      <th>nunique</th>\n",
       "      <th>5th_percentile</th>\n",
       "      <th>10th_percentile</th>\n",
       "      <th>25th_percentile</th>\n",
       "      <th>50th_percentile</th>\n",
       "      <th>75th_percentile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>-17.35</td>\n",
       "      <td>39.126</td>\n",
       "      <td>10.645179</td>\n",
       "      <td>2.358588e+06</td>\n",
       "      <td>7.319994</td>\n",
       "      <td>53.582319</td>\n",
       "      <td>10.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51018</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>14.646173</td>\n",
       "      <td>214.510397</td>\n",
       "      <td>24.56076</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>103270</td>\n",
       "      <td>1.00248</td>\n",
       "      <td>4.448505</td>\n",
       "      <td>14.42916</td>\n",
       "      <td>24.56076</td>\n",
       "      <td>32.921443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp                                                        \\\n",
       "                min     max       mean           sum       std        var   \n",
       "station_code                                                                \n",
       "DENW094      -17.35  39.126  10.645179  2.358588e+06  7.319994  53.582319   \n",
       "\n",
       "                                                 ...         o3              \\\n",
       "             median prod nunique 5th_percentile  ...        std         var   \n",
       "station_code                                     ...                          \n",
       "DENW094       10.45  NaN   51018          -0.75  ...  14.646173  214.510397   \n",
       "\n",
       "                                                                    \\\n",
       "                median prod nunique 5th_percentile 10th_percentile   \n",
       "station_code                                                         \n",
       "DENW094       24.56076 -0.0  103270        1.00248        4.448505   \n",
       "\n",
       "                                                              \n",
       "             25th_percentile 50th_percentile 75th_percentile  \n",
       "station_code                                                  \n",
       "DENW094             14.42916        24.56076       32.921443  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'min'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>-17.35</td>\n",
       "      <td>-1.417507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp        o3\n",
       "station_code                 \n",
       "DENW094      -17.35 -1.417507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'max'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>39.126</td>\n",
       "      <td>134.83356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                temp         o3\n",
       "station_code                   \n",
       "DENW094       39.126  134.83356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mean'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>10.645179</td>\n",
       "      <td>24.565799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   temp         o3\n",
       "station_code                      \n",
       "DENW094       10.645179  24.565799"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'std'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>7.319994</td>\n",
       "      <td>14.646173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  temp         o3\n",
       "station_code                     \n",
       "DENW094       7.319994  14.646173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = ['min', 'max', 'mean', 'sum', 'std', 'var', 'median','prod','nunique',\n",
    "    ('5th_percentile', lambda x: x.quantile(0.05)),\n",
    "    ('10th_percentile', lambda x: x.quantile(0.10)),\n",
    "    ('25th_percentile', lambda x: x.quantile(0.25)),\n",
    "    ('50th_percentile', lambda x: x.quantile(0.50)), #(median)\n",
    "    ('75th_percentile', lambda x: x.quantile(0.75))]\n",
    "agg_dict = {col: stats for col in variable_columns}\n",
    "grouped = dataframes.groupby('station_code').agg(agg_dict)\n",
    "display(grouped)\n",
    "\n",
    "for agg_func in ['min', 'max', 'mean', 'std']:\n",
    "    display(agg_func)\n",
    "    agg_view = grouped.xs(agg_func, axis=1, level=1)\n",
    "    display(agg_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bad453c7",
   "metadata": {
    "id": "bad453c7"
   },
   "outputs": [],
   "source": [
    "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hkSc-_CdSxyZ",
   "metadata": {
    "id": "hkSc-_CdSxyZ"
   },
   "source": [
    "#### Log Scaling after checking for skewdness\n",
    "\n",
    "*ToDo:* This cell should be transfered to the data preparation part of the ML models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "yNPu71utSxS2",
   "metadata": {
    "id": "yNPu71utSxS2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "def log_transform_if_skewed(df, columns, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Log-transform the specified columns of a DataFrame based on their skewdness.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names that need to be checked for skewdness.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized columns.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    for col in columns:\n",
    "        # s = df[col].dropna()\n",
    "        s = df[col]\n",
    "        current_skewness = skew(s)\n",
    "\n",
    "        print(f\"[{col}] Skewness: {current_skewness:.2f}\")\n",
    "\n",
    "        if abs(current_skewness) > threshold:\n",
    "            # To avoid log(0) or log(negative values).\n",
    "            if (s <= 0).any():\n",
    "                shift = abs(s.min()) + 1e-6\n",
    "                print(f\"Applying log(x + {shift:.6f}) to {col}\")\n",
    "                df_transformed[col] = np.log(df[col] + shift)\n",
    "            else:\n",
    "                print(f\"Applying log(x) to {col}\")\n",
    "                df_transformed[col] = np.log(df[col])\n",
    "        else:\n",
    "            print(f\"No transformation applied to {col}.\")\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "MeI64XXFXED8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "MeI64XXFXED8",
    "outputId": "35d8be00-8882-49ee-9ebc-f8b6845b919a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temp] Skewness: 0.17\n",
      "No transformation applied to temp.\n",
      "[o3] Skewness: 0.62\n",
      "No transformation applied to o3.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>-15.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>-16.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>-16.45</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>-17.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude   latitude station_code                  datetime   temp   o3\n",
       "0   6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 -15.85  0.0\n",
       "1   6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 -16.35  0.0\n",
       "2   6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 -16.45  0.0\n",
       "3   6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 -17.15  0.0\n",
       "4   6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 -17.35  0.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_= log_transform_if_skewed(dataframes, variable_columns, threshold=1.0)\n",
    "dataframe_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ojsWf0KrRyjr",
   "metadata": {
    "id": "ojsWf0KrRyjr"
   },
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z7XAF2WzcI5G",
   "metadata": {
    "id": "z7XAF2WzcI5G"
   },
   "source": [
    "ðŸ˜ˆ Task 8: Why do we need both log transformation and Z-score normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "66cdb9b1-a453-4575-a87f-c474c0a1f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.to_csv(\"not_normalized_data_multi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1d2c0b47",
   "metadata": {
    "id": "1d2c0b47"
   },
   "outputs": [],
   "source": [
    "def standard_scaler(df, columns):\n",
    "    \"\"\"\n",
    "    Standardize the specified columns of a DataFrame by subtracting the mean\n",
    "    and dividing by the standard deviation (Z-score normalization).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized columns.\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    for col in columns:\n",
    "        mean = df_scaled[col].mean()\n",
    "        print(mean)\n",
    "        std = df_scaled[col].std()\n",
    "        df_scaled[col] = (df_scaled[col] - mean) / std\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6kGKgXpsRzgk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6kGKgXpsRzgk",
    "outputId": "6c98c637-07d8-4f7f-cf4d-772ff4c21ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.645179247129887\n",
      "24.565799430301844\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>station_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>-3.619563</td>\n",
       "      <td>-1.677284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>-3.687869</td>\n",
       "      <td>-1.677284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>-3.701530</td>\n",
       "      <td>-1.677284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>-3.797159</td>\n",
       "      <td>-1.677284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.093923</td>\n",
       "      <td>50.754704</td>\n",
       "      <td>DENW094</td>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>-3.824481</td>\n",
       "      <td>-1.677284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude   latitude station_code                  datetime      temp  \\\n",
       "0   6.093923  50.754704      DENW094 1997-01-01 00:00:00+00:00 -3.619563   \n",
       "1   6.093923  50.754704      DENW094 1997-01-01 01:00:00+00:00 -3.687869   \n",
       "2   6.093923  50.754704      DENW094 1997-01-01 02:00:00+00:00 -3.701530   \n",
       "3   6.093923  50.754704      DENW094 1997-01-01 03:00:00+00:00 -3.797159   \n",
       "4   6.093923  50.754704      DENW094 1997-01-01 04:00:00+00:00 -3.824481   \n",
       "\n",
       "         o3  \n",
       "0 -1.677284  \n",
       "1 -1.677284  \n",
       "2 -1.677284  \n",
       "3 -1.677284  \n",
       "4 -1.677284  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = standard_scaler(dataframe_, variable_columns)\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-cc-ixdbc_QR",
   "metadata": {
    "id": "-cc-ixdbc_QR"
   },
   "source": [
    "##### Save the normalized dataframe for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "97c17924-a43c-4aa3-8df8-c96b1b00c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.to_csv(\"normalized_data_multi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3tHSnChQeO-L",
   "metadata": {
    "id": "3tHSnChQeO-L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221564, 6)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c7c08-54ed-4c9c-9fb0-709a0ac15877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2588a2f2",
    "7426db96"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cudaenv]",
   "language": "python",
   "name": "conda-env-cudaenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
